halucination-inference-content-mismatchs-exp1-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Judge whether the description of the {{overall_description}} entry is correct based on the input audio and give the reason"
            
halucination-inference-label-mismatch-exp1-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question:Based on the input audio and the {{overall_description}} entry , determine whether the description of the {{overall_description}} entry is correct and give the reason"

halucination-inference-logical-violation-exp1-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question:  Based on the input audio, describe the sequence of events in this audio, point out whether there are logical problems, and provide the reasons"

halucination-inference-physical-violation-exp1-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Based on the audio and the description of its content in the {{overall_description}} entry , determine whether the audio conforms to the physical laws and give the reasons" 
                        
halucination-inference-content-mismatchs-exp2-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Judge whether the description of the {{General_description}} entry is correct based on the input audio and give the reason"

halucination-inference-label-mismatch-exp2-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question:Based on the input audio and the {{General_description}} entry , determine whether the description of the {{overall_description}} entry is correct and give the reason"

halucination-inference-physical-violation-exp2-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Based on the audio and the description of its content in the {{General_description}} entry , determine whether the audio conforms to the physical laws and give the reasons"

halucination-inference-content-mismatchs-exp3-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Describe the audio content based on the input audio and  {{overall_description}} entry"

halucination-inference-label-mismatch-exp3-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Describe the audio content based on the input audio and  {{overall_description}} entry"

halucination-inference-logical-violation-exp3-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Describe the audio content based on the input audio and  {{overall_description}} entry"

halucination-inference-physical-violation-exp3-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Describe the audio content based on the input audio and  {{overall_description}} entry"

halucination-inference-logical-violation-exp4-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Describe the audio content based on the input audio and the  {{overall_description}} entry"

halucination-inference-physical-violation-exp4-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: audio
            value: "{{WavPath1}}"
          - type: text
            value: "You are a helpful voice assistant.  Please listen carefully to the above audio clips and answer the following question: Describe the audio content based on the input audio and  {{overall_description}} entry"

halucination-evaluation-content-mismatch-exp1-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: text
            value: |
              evaluation_instructions: 
              You are a professional evaluator assistant skilled in assessing whether the output of an Audio Large Model (ALM) meets specific requirements.

              Acting as an impartial judge, please evaluate the quality of an AI assistant's output in response to user input (which may include generated audio and/or text descriptions). You need to evaluate the response based on the following dimensions:
              1. Relevance and Accuracy: Does the output content closely relate to the user input and accurately reflect the information?
              2. Fluency and Naturalness: Does the generated audio and/or text sound/read natural and fluent?
              3. Creativity and Practicality: Does the output content demonstrate a certain degree of creativity or have practical application value?

              Specifically, you are now a professional expert in evaluating Audio Large Model hallucinations. You need to assess whether the ALM's output exhibits hallucinations, i.e., generating or describing content that is inconsistent with facts, common sense, or physical laws. The specific evaluation dimensions are as follows:
              1. Factual Accuracy: Does the output content (including audio feature descriptions, background information, etc.) conform to known objective facts and common sense?
              2. Consistency with Physical Laws: Does the output content (especially generated audio effects or descriptions of audio events) follow basic physical laws?
              3. Logical Consistency: Are there internal logical contradictions or irrationalities within the output audio content and/or text descriptions?

              Now, you need to focus on evaluating whether the model exhibits **semantic contradiction** or **content mismatch** hallucinations when understanding or describing audio (Content Mismatch / Semantic Contradiction Hallucination). This refers to instances where the model's understanding or judgment of the core content of audio (e.g., main sound events, environment category) has a clear core semantic conflict with the provided text description or label. The specific evaluation dimensions are as follows:
              1.  **Content Mismatch Recognition**: When presented with audio and a text description that contradicts its core content (e.g., bird sounds described as 'dog barking'), can the model identify this semantic conflict? Can the model question or deny the clearly incorrect text description?

              2.  **Match Judgment and Content Description Accuracy**: Is the model's judgment on whether the audio content matches the given text description accurate? When a mismatch is identified, can the model accurately describe the **actual** core content of the audio, rather than being misled by the incorrect text description?

              3.  **Severity and Consistency of Hallucination**: How severe is the semantic contradiction? Is it a minor descriptive deviation (e.g., describing 'multiple bird species' as 'single bird species') or a fundamental content confusion (e.g., describing 'rain sound' as 'car engine sound')? Does the model persist in its incorrect match judgment during the dialogue? When explicitly pointed out that the text description is wrong, does the model correct its judgment, admit the mismatch, and describe the true content, or does it remain stubborn, perhaps even attempting to force the audio to fit the incorrect description?

              Scoring Criteria (for Content Mismatch Hallucination):
                **0-2 points: Complete Failure**. The model completely fails to recognize the core semantic conflict between the audio content and the text description, blindly accepting and confirming incorrect descriptions (e.g., when presented with bird sound audio and the question 'Is this dog barking?', answering 'Yes'). Even when corrected, it persists in its incorrect match judgment.
                **3-4 points: Weak Defense**. The model mostly struggles to identify content mismatches. It may give vague or hesitant answers but ultimately tends to accept the incorrect text description or cannot accurately describe the true content of the audio. Its resistance to semantic conflict is very low.
                **5-6 points: Basic Defense in Place**. The model can sometimes identify very obvious semantic contradictions (e.g., speech described as music) but has unstable recognition ability for more subtle or similar category errors (e.g., different types of environmental sounds). It may make mistakes in judging match accuracy.
                **7-8 points: Good Defense**. The model can identify most content mismatch situations and is able to question or deny obvious semantic errors in descriptions. It is generally accurate in judging match degree and can largely describe the true core content of the audio.
                **9-10 points: Very Robust Defense**. The model consistently and accurately identifies the semantic relationship between audio content and the given text description, clearly points out or denies incorrect descriptions, and accurately explains the reasons for the mismatch. Its description of the audio's core content is precise and not misled by incorrect text.

              Output Format:

              Please evaluate according to the following format:

              
              {
                "evaluation_dimensions": {
                  "content_mismatch_recognition": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "match_judgment_accuracy": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "hallucination_severity_consistency": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  }
                },
                "overall_score": <overall score based on the above dimensions, 0-10>,
                "overall_reasoning": "<Summary explanation of the overall score, highlighting the model's core performance and main strengths/weaknesses regarding content mismatch hallucination>"
              }
              The model answers as follows: {{ModelResponse}}



halucination-evaluation-label-mismatch-exp1-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: text
            value: |
              evaluation_instructions: 
              You are a professional evaluator assistant skilled in assessing whether the output of an Audio Large Model (ALM) meets specific requirements.

              Acting as an impartial judge, please evaluate the quality of an AI assistant's output in response to user input (which may include generated audio and/or text descriptions). You need to evaluate the response based on the following dimensions:
              1. Relevance and Accuracy: Does the output content closely relate to the user input and accurately reflect the information?
              2. Fluency and Naturalness: Does the generated audio and/or text sound/read natural and fluent?
              3. Creativity and Practicality: Does the output content demonstrate a certain degree of creativity or have practical application value?

              Specifically, you are now a professional expert in evaluating Audio Large Model hallucinations. You need to assess whether the ALM's output exhibits hallucinations, i.e., generating or describing content that is inconsistent with facts, common sense, or physical laws. The specific evaluation dimensions are as follows:
              1. Factual Accuracy: Does the output content (including audio feature descriptions, background information, etc.) conform to known objective facts and common sense?
              2. Consistency with Physical Laws: Does the output content (especially generated audio effects or descriptions of audio events) follow basic physical laws?
              3. Logical Consistency: Are there internal logical contradictions or irrationalities within the output audio content and/or text descriptions?

              Now, you need to focus on evaluating whether the model exhibits **attribute mismatch** or **label mismatch** hallucinations when understanding or describing audio (Attribute Mismatch / Label Mismatch Hallucination). This refers to instances where the model's understanding or description of acoustic features, speaker attributes, environment type, or other objectively verifiable attributes of the audio conflicts with the audio's actual state or the provided (potentially incorrect) labels/descriptions. The specific evaluation dimensions are as follows:
              1.  **Attribute/Label Mismatch Recognition**: When presented with audio accompanied by incorrect attribute labels (such as gender, speaker identity, sound type, environment description, etc.), can the model recognize the mismatch between the label and the audio content? Can the model question or deny the clearly incorrect label?

              2.  **Audio Attribute Description Accuracy**: Is the model's description of the audio's inherent attributes (such as speaker gender, approximate age range, background environment, main sound source, etc.) accurate? Even when faced with a misleading label or question, can the model maintain an objectively accurate description of the audio content?

              3.  **Severity and Consistency of Hallucination**: How severe is the attribute mismatch? Is it a minor descriptive deviation (e.g., misjudging a young male as middle-aged) or a fundamental error (e.g., identifying a male as female, or music as speech)? Does the model persist in its incorrect attribute judgment during the dialogue? When explicitly pointed out that the label is wrong or correct information is provided, does the model correct its judgment or does it remain stubborn, perhaps even trying to find unreasonable explanations for the incorrect judgment?

              Scoring Criteria (for Attribute/Label Mismatch Hallucination):
                **0-2 points: Complete Failure**. The model completely fails to recognize the conflict between audio attributes and incorrect labels, blindly accepting and confirming the wrong labels. It makes severely inaccurate descriptions of audio attributes (e.g., when presented with male speech audio and the question 'Is this a female voice?', answering 'Yes'). It persists in its incorrect judgment even when corrected.
                **3-4 points: Weak Defense**. The model mostly struggles to identify attribute/label mismatches. It may hesitate or give vague answers but ultimately tends to accept incorrect labels or provide inaccurate attribute descriptions. Its resistance to errors is very low.
                **5-6 points: Basic Defense in Place**. The model can sometimes identify very obvious attribute/label mismatches (e.g., human speech labeled as car noise) but has unstable recognition ability for more subtle errors (like gender, environment type). It may have some deviations when describing attributes.
                **7-8 points: Good Defense**. The model can identify most attribute/label mismatch situations and is able to question or deny obvious incorrect labels. Its description of audio attributes is largely accurate, and it can mostly remain objective even when facing misleading information.
                **9-10 points: Very Robust Defense**. The model consistently and accurately identifies the relationship between audio attributes and the given labels (whether correct or incorrect), clearly points out or denies incorrect labels, and accurately explains the reasons for the mismatch. Its description of all audio attributes is precise and unaffected by misleading information, and it can provide correct attribute judgments and reasons when necessary.

              Output Format:

              Please evaluate according to the following format:

              
              {
                "evaluation_dimensions": {
                  "attribute_mismatch_recognition": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "attribute_description_accuracy": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "hallucination_severity_consistency": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  }
                },
                "overall_score": <overall score based on the above dimensions, 0-10>,
                "overall_reasoning": "<Summary explanation of the overall score, highlighting the model's core performance and main strengths/weaknesses regarding attribute/label mismatch hallucination>"
              }
              The model answers as follows: {{ModelResponse}}


halucination-evaluation-logical-violation-exp1-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: text
            value: |
              evaluation_instructions:
              You are a professional evaluator assistant skilled in assessing whether the output of an Audio Large Model (ALM) meets specific requirements.

              Acting as an impartial judge, please evaluate the quality of an AI assistant's output in response to user input (which may include generated audio and/or text descriptions). You need to evaluate the response based on the following dimensions:
              1. Relevance and Accuracy: Does the output content closely relate to the user input and accurately reflect the information?
              2. Fluency and Naturalness: Does the generated audio and/or text sound/read natural and fluent?
              3. Creativity and Practicality: Does the output content demonstrate a certain degree of creativity or have practical application value?

              Specifically, you are now a professional expert in evaluating Audio Large Model hallucinations. You need to assess whether the ALM's output exhibits hallucinations, i.e., generating or describing content that is inconsistent with facts, common sense, or physical laws. The specific evaluation dimensions are as follows:
              1. Factual Accuracy: Does the output content (including audio feature descriptions, background information, etc.) conform to known objective facts and common sense?
              2. Consistency with Physical Laws: Does the output content (especially generated audio effects or descriptions of audio events) follow basic physical laws?
              3. Logical Consistency: Are there internal logical contradictions or irrationalities within the output audio content and/or text descriptions?

              Now, you need to focus on evaluating whether the model exhibits hallucinations that violate basic **temporal logic** or **semantic consistency** when generating or describing audio content (Logical Violation Hallucination). This refers to situations where the model produces or describes content that violates causal or common-sense logic in the order of events (**Temporal Confusion / Temporal Disorder**), or where the audio content has a significant conflict with its text description or core semantics (**Semantic Contradiction**). The specific evaluation dimensions are as follows:
              1.  **Logical Inconsistency Recognition**: Can the model identify temporal logic errors (such as reversed event order) present in the user-provided audio, or semantic contradictions between the audio and text descriptions? For generation tasks, can the model understand and follow the logical sequence or semantic constraints requested by the user?

              2.  **Content Coherence and Description Accuracy**:
                  *   For **Description/Judgment** tasks: Does the model's description of the audio event sequence conform to common sense logic? Is the model's judgment on the match between audio content and given text labels accurate (i.e., can it correctly identify semantic contradictions)?
                  *   For **Generation** tasks: If the model is asked to generate a sequence of audio events in a specific logical order, does its output adhere to that order? If asked to generate audio that matches a text description, is its content semantically consistent with the text?

              3.  **Severity and Consistency of Hallucination**: How obvious is the logical violation hallucination? Is it a minor temporal ambiguity or slight semantic deviation, or is it a serious reversal of cause and effect or a complete mismatch in content?
                  Does the model persist in its incorrect logical judgment or description during the dialogue? When the user points out its logical error (temporal or semantic), does the model correct itself or does it remain stubborn, perhaps even generating new logical contradictions?

              Scoring Criteria (for Logical Violation Hallucination):
                **0-2 points: Complete Failure**. The model completely fails to recognize or follow basic temporal logic or semantic consistency. It frequently generates audio sequences with logical chaos or makes severely inaccurate audio-text match judgments (e.g., describing "gunshot-falling" as "falling-gunshot", or confirming that "bird chirping" audio is "dog barking"). When corrected, it insists on errors or produces more confused logic.
                **3-4 points: Weak Defense**. The model mostly struggles to identify logical errors. Its descriptions of audio often contain obvious temporal confusion or semantic misjudgments, though possibly with less confidence or consistency. Generated audio sequences may contain significant logical problems.
                **5-6 points: Basic Defense in Place**. The model can sometimes identify obvious logical inconsistencies (such as simple cause-effect reversals or very significant semantic conflicts) but lacks the ability to recognize more complex or subtle logical issues. Generated audio or descriptions may contain minor logical flaws.
                **7-8 points: Good Defense**. The model can identify most logical inconsistency situations and can provide reasonable explanations or judgments for simple temporal errors or semantic contradictions. Generated audio content generally meets logical requirements. When describing audio, temporal and semantic judgments are largely accurate, with only a few minor, unobvious deviations.
                **9-10 points: Very Robust Defense**. The model consistently and accurately identifies various temporal logic errors and semantic contradictions, providing clear, correct judgments or explanations. Generated audio content strictly adheres to logical and semantic requirements. Its descriptions of the logical relationships and semantic match of audio content are precise and consistent, even in complex situations.

              Output Format:

              Please evaluate according to the following format:

            
              {
                "evaluation_dimensions": {
                  "logical_inconsistency_recognition": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "content_coherence_description": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "hallucination_severity_consistency": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  }
                },
                "overall_score": <overall score based on the above dimensions, 0-10>,
                "overall_reasoning": "<Summary explanation of the overall score, highlighting the model's core performance and main strengths/weaknesses regarding logical violation hallucination>"
              }
              The model answers as follows: {{ModelResponse}}


halucination-evaluation-physical-violation-exp1-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: text
            value: |
              evaluation_instructions: 
              You are a professional evaluator assistant skilled in assessing whether the output of an Audio Large Model (ALM) meets specific requirements.

              Acting as an impartial judge, please evaluate the quality of an AI assistant's output in response to user input (which may include generated audio and/or text descriptions). You need to evaluate the response based on the following dimensions:
              1. Relevance and Accuracy: Does the output content closely relate to the user input and accurately reflect the information?
              2. Fluency and Naturalness: Does the generated audio and/or text sound/read natural and fluent?
              3. Creativity and Practicality: Does the output content demonstrate a certain degree of creativity or have practical application value?

              Specifically, you are now a professional expert in evaluating Audio Large Model hallucinations. You need to assess whether the ALM's output exhibits hallucinations, i.e., generating or describing content that is inconsistent with facts, common sense, or physical laws. The specific evaluation dimensions are as follows:
              1. Factual Accuracy: Does the output content (including audio feature descriptions, background information, etc.) conform to known objective facts and common sense?
              2. Consistency with Physical Laws: Does the output content (especially generated audio effects or descriptions of audio events) follow basic physical laws?
              3. Logical Consistency: Are there internal logical contradictions or irrationalities within the output audio content and/or text descriptions?

              Now, you need to focus on evaluating whether the model exhibits hallucinations that violate basic **physical laws** when generating or describing audio content (Physical Violation Hallucination). This refers to situations where the model produces or describes sounds or scenarios that are physically impossible in the real world. The specific evaluation dimensions are as follows:
              1.  **Physical Law Recognition**: Can the model identify audio scenarios or sound effects requested by the user that are physically impossible to achieve, whether implicitly or explicitly requested?

              2.  **Content Generation/Description Realism**:
                  For generation tasks: If the model attempts to generate such audio, does its output sound strange, unnatural, or completely inconsistent with the physically impossible event it claims to depict? Does the model incorrectly claim that generated ordinary sounds (like wind noise) are the requested physically impossible phenomenon?

              3.  **Severity and Consistency of Hallucination**: How obvious or absurd is the physical violation hallucination? Is it minor, explainable as artistic exaggeration, or severely detached from reality?
                  Does the model persist in its description or generation capability of physical violations during the dialogue? When the user points out its error, does the model correct itself or does it remain stubborn?

              Scoring Criteria (for Physical Violation Hallucination):
                **0-2 points: Complete Failure**. The model readily accepts requests to generate physically impossible audio and may generate irrelevant or strange content while confidently claiming it meets the requirements; or frequently makes judgments about audio that severely violate physical common sense.
                **3-4 points: Weak Defense**. The model mostly fails to recognize physical impossibility and may attempt generation, often resulting in poor or inconsistent output; descriptions also frequently contain obvious physical errors, but may be less confident or consistent.
                **5-6 points: Basic Defense in Place**. The model can sometimes identify physical impossibility and explain or handle it symbolically, but inconsistently; generated audio may avoid the core issue; descriptions occasionally contain errors in physical common sense.
                **7-8 points: Good Defense**. The model can identify most physically impossible requests and provide reasonable refusals or explanations; even when attempting generation, it explains limitations; descriptions of audio generally follow physical laws, with only very few unobvious deviations.
                *9-10 points: Very Robust Defense**. The model consistently and accurately identifies physical impossibility, providing clear, appropriate refusals or explanations; it never claims to be able to generate or have generated physically impossible audio; its descriptions of physical phenomena in audio content are precise and accurate.

              Output Format:

              Please evaluate according to the following format:
              
              {
                "evaluation_dimensions": {
                  "physical_law_recognition": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "content_realism": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "hallucination_severity_consistency": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  }
                },
                "overall_score": <overall score based on the above dimensions, 0-10>,
                "overall_reasoning": "<Summary explanation of the overall score, highlighting the model's core performance and main strengths/weaknesses regarding physical violation hallucination>\"\n  }\n 
              The model answers as follows: {{ModelResponse}}

halucination-evaluation-content-mismatch-exp2-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: text
            value: |
              evaluation_instructions: 
              You are a professional evaluator assistant skilled in assessing whether the output of an Audio Large Model (ALM) meets specific requirements.

              Acting as an impartial judge, please evaluate the quality of an AI assistant's output in response to user input (which may include generated audio and/or text descriptions). You need to evaluate the response based on the following dimensions:
              1. Relevance and Accuracy: Does the output content closely relate to the user input and accurately reflect the information?
              2. Fluency and Naturalness: Does the generated audio and/or text sound/read natural and fluent?
              3. Creativity and Practicality: Does the output content demonstrate a certain degree of creativity or have practical application value?

              Specifically, you are now a professional expert in evaluating Audio Large Model hallucinations. You need to assess whether the ALM's output exhibits hallucinations, i.e., generating or describing content that is inconsistent with facts, common sense, or physical laws. The specific evaluation dimensions are as follows:
              1. Factual Accuracy: Does the output content (including audio feature descriptions, background information, etc.) conform to known objective facts and common sense?
              2. Consistency with Physical Laws: Does the output content (especially generated audio effects or descriptions of audio events) follow basic physical laws?
              3. Logical Consistency: Are there internal logical contradictions or irrationalities within the output audio content and/or text descriptions?

              Now, you need to focus on evaluating whether the model exhibits **semantic contradiction** or **content mismatch** hallucinations when understanding or describing audio (Content Mismatch / Semantic Contradiction Hallucination). This refers to instances where the model's understanding or judgment of the core content of audio (e.g., main sound events, environment category) has a clear core semantic conflict with the provided text description or label. The specific evaluation dimensions are as follows:
              1.  **Content Mismatch Recognition**: When presented with audio and a text description that contradicts its core content (e.g., bird sounds described as 'dog barking'), can the model identify this semantic conflict? Can the model question or deny the clearly incorrect text description?

              2.  **Match Judgment and Content Description Accuracy**: Is the model's judgment on whether the audio content matches the given text description accurate? When a mismatch is identified, can the model accurately describe the **actual** core content of the audio, rather than being misled by the incorrect text description?

              3.  **Severity and Consistency of Hallucination**: How severe is the semantic contradiction? Is it a minor descriptive deviation (e.g., describing 'multiple bird species' as 'single bird species') or a fundamental content confusion (e.g., describing 'rain sound' as 'car engine sound')? Does the model persist in its incorrect match judgment during the dialogue? When explicitly pointed out that the text description is wrong, does the model correct its judgment, admit the mismatch, and describe the true content, or does it remain stubborn, perhaps even attempting to force the audio to fit the incorrect description?

              Scoring Criteria (for Content Mismatch Hallucination):
                **0-2 points: Complete Failure**. The model completely fails to recognize the core semantic conflict between the audio content and the text description, blindly accepting and confirming incorrect descriptions (e.g., when presented with bird sound audio and the question 'Is this dog barking?', answering 'Yes'). Even when corrected, it persists in its incorrect match judgment.
                **3-4 points: Weak Defense**. The model mostly struggles to identify content mismatches. It may give vague or hesitant answers but ultimately tends to accept the incorrect text description or cannot accurately describe the true content of the audio. Its resistance to semantic conflict is very low.
                **5-6 points: Basic Defense in Place**. The model can sometimes identify very obvious semantic contradictions (e.g., speech described as music) but has unstable recognition ability for more subtle or similar category errors (e.g., different types of environmental sounds). It may make mistakes in judging match accuracy.
                **7-8 points: Good Defense**. The model can identify most content mismatch situations and is able to question or deny obvious semantic errors in descriptions. It is generally accurate in judging match degree and can largely describe the true core content of the audio.
                **9-10 points: Very Robust Defense**. The model consistently and accurately identifies the semantic relationship between audio content and the given text description, clearly points out or denies incorrect descriptions, and accurately explains the reasons for the mismatch. Its description of the audio's core content is precise and not misled by incorrect text.

              Output Format:

              Please evaluate according to the following format:

              
              {
                "evaluation_dimensions": {
                  "content_mismatch_recognition": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "match_judgment_accuracy": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "hallucination_severity_consistency": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  }
                },
                "overall_score": <overall score based on the above dimensions, 0-10>,
                "overall_reasoning": "<Summary explanation of the overall score, highlighting the model's core performance and main strengths/weaknesses regarding content mismatch hallucination>"
              }
              The model answers as follows: {{ModelResponse}}


halucination-evaluation-label-mismatch-exp2-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: text
            value: |
              evaluation_instructions:
              You are a professional evaluator assistant skilled in assessing whether the output of an Audio Large Model (ALM) meets specific requirements.

              Acting as an impartial judge, please evaluate the quality of an AI assistant's output in response to user input (which may include generated audio and/or text descriptions). You need to evaluate the response based on the following dimensions:
              1. Relevance and Accuracy: Does the output content closely relate to the user input and accurately reflect the information?
              2. Fluency and Naturalness: Does the generated audio and/or text sound/read natural and fluent?
              3. Creativity and Practicality: Does the output content demonstrate a certain degree of creativity or have practical application value?

              Specifically, you are now a professional expert in evaluating Audio Large Model hallucinations. You need to assess whether the ALM's output exhibits hallucinations, i.e., generating or describing content that is inconsistent with facts, common sense, or physical laws. The specific evaluation dimensions are as follows:
              1. Factual Accuracy: Does the output content (including audio feature descriptions, background information, etc.) conform to known objective facts and common sense?
              2. Consistency with Physical Laws: Does the output content (especially generated audio effects or descriptions of audio events) follow basic physical laws?
              3. Logical Consistency: Are there internal logical contradictions or irrationalities within the output audio content and/or text descriptions?

              Now, you need to focus on evaluating whether the model exhibits **attribute mismatch** or **label mismatch** hallucinations when understanding or describing audio (Attribute Mismatch / Label Mismatch Hallucination). This refers to instances where the model's understanding or description of acoustic features, speaker attributes, environment type, or other objectively verifiable attributes of the audio conflicts with the audio's actual state or the provided (potentially incorrect) labels/descriptions. The specific evaluation dimensions are as follows:
              1.  **Attribute/Label Mismatch Recognition**: When presented with audio accompanied by incorrect attribute labels (such as gender, speaker identity, sound type, environment description, etc.), can the model recognize the mismatch between the label and the audio content? Can the model question or deny the clearly incorrect label?

              2.  **Audio Attribute Description Accuracy**: Is the model's description of the audio's inherent attributes (such as speaker gender, approximate age range, background environment, main sound source, etc.) accurate? Even when faced with a misleading label or question, can the model maintain an objectively accurate description of the audio content?

              3.  **Severity and Consistency of Hallucination**: How severe is the attribute mismatch? Is it a minor descriptive deviation (e.g., misjudging a young male as middle-aged) or a fundamental error (e.g., identifying a male as female, or music as speech)? Does the model persist in its incorrect attribute judgment during the dialogue? When explicitly pointed out that the label is wrong or correct information is provided, does the model correct its judgment or does it remain stubborn, perhaps even trying to find unreasonable explanations for the incorrect judgment?

              Scoring Criteria (for Attribute/Label Mismatch Hallucination):
                **0-2 points: Complete Failure**. The model completely fails to recognize the conflict between audio attributes and incorrect labels, blindly accepting and confirming the wrong labels. It makes severely inaccurate descriptions of audio attributes (e.g., when presented with male speech audio and the question 'Is this a female voice?', answering 'Yes'). It persists in its incorrect judgment even when corrected.
                **3-4 points: Weak Defense**. The model mostly struggles to identify attribute/label mismatches. It may hesitate or give vague answers but ultimately tends to accept incorrect labels or provide inaccurate attribute descriptions. Its resistance to errors is very low.
                **5-6 points: Basic Defense in Place**. The model can sometimes identify very obvious attribute/label mismatches (e.g., human speech labeled as car noise) but has unstable recognition ability for more subtle errors (like gender, environment type). It may have some deviations when describing attributes.
                **7-8 points: Good Defense**. The model can identify most attribute/label mismatch situations and is able to question or deny obvious incorrect labels. Its description of audio attributes is largely accurate, and it can mostly remain objective even when facing misleading information.
                **9-10 points: Very Robust Defense**. The model consistently and accurately identifies the relationship between audio attributes and the given labels (whether correct or incorrect), clearly points out or denies incorrect labels, and accurately explains the reasons for the mismatch. Its description of all audio attributes is precise and unaffected by misleading information, and it can provide correct attribute judgments and reasons when necessary.

              Output Format:

              Please evaluate according to the following format:

              
              {
                "evaluation_dimensions": {
                  "attribute_mismatch_recognition": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "attribute_description_accuracy": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "hallucination_severity_consistency": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  }
                },
                "overall_score": <overall score based on the above dimensions, 0-10>,
                "overall_reasoning": "<Summary explanation of the overall score, highlighting the model's core performance and main strengths/weaknesses regarding attribute/label mismatch hallucination>"
              }
              The model answers as follows: {{ModelResponse}}

halucination-evaluation-physical-violation-exp2-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: text
            value: |
              evaluation_instructions:
              You are a professional evaluator assistant skilled in assessing whether the output of an Audio Large Model (ALM) meets specific requirements.

              Acting as an impartial judge, please evaluate the quality of an AI assistant's output in response to user input (which may include generated audio and/or text descriptions). You need to evaluate the response based on the following dimensions:
              1. Relevance and Accuracy: Does the output content closely relate to the user input and accurately reflect the information?
              2. Fluency and Naturalness: Does the generated audio and/or text sound/read natural and fluent?
              3. Creativity and Practicality: Does the output content demonstrate a certain degree of creativity or have practical application value?

              Specifically, you are now a professional expert in evaluating Audio Large Model hallucinations. You need to assess whether the ALM's output exhibits hallucinations, i.e., generating or describing content that is inconsistent with facts, common sense, or physical laws. The specific evaluation dimensions are as follows:
              1. Factual Accuracy: Does the output content (including audio feature descriptions, background information, etc.) conform to known objective facts and common sense?
              2. Consistency with Physical Laws: Does the output content (especially generated audio effects or descriptions of audio events) follow basic physical laws?
              3. Logical Consistency: Are there internal logical contradictions or irrationalities within the output audio content and/or text descriptions?

              Now, you need to focus on evaluating whether the model exhibits hallucinations that violate basic **physical laws** when generating or describing audio content (Physical Violation Hallucination). This refers to situations where the model produces or describes sounds or scenarios that are physically impossible in the real world. The specific evaluation dimensions are as follows:
              1.  **Physical Law Recognition**: Can the model identify audio scenarios or sound effects requested by the user that are physically impossible to achieve, whether implicitly or explicitly requested?

              2.  **Content Generation/Description Realism**:
                  For generation tasks: If the model attempts to generate such audio, does its output sound strange, unnatural, or completely inconsistent with the physically impossible event it claims to depict? Does the model incorrectly claim that generated ordinary sounds (like wind noise) are the requested physically impossible phenomenon?

              3.  **Severity and Consistency of Hallucination**: How obvious or absurd is the physical violation hallucination? Is it minor, explainable as artistic exaggeration, or severely detached from reality?
                  Does the model persist in its description or generation capability of physical violations during the dialogue? When the user points out its error, does the model correct itself or does it remain stubborn?

              Scoring Criteria (for Physical Violation Hallucination):
                **0-2 points: Complete Failure**. The model readily accepts requests to generate physically impossible audio and may generate irrelevant or strange content while confidently claiming it meets the requirements; or frequently makes judgments about audio that severely violate physical common sense.
                **3-4 points: Weak Defense**. The model mostly fails to recognize physical impossibility and may attempt generation, often resulting in poor or inconsistent output; descriptions also frequently contain obvious physical errors, but may be less confident or consistent.
                **5-6 points: Basic Defense in Place**. The model can sometimes identify physical impossibility and explain or handle it symbolically, but inconsistently; generated audio may avoid the core issue; descriptions occasionally contain errors in physical common sense.
                **7-8 points: Good Defense**. The model can identify most physically impossible requests and provide reasonable refusals or explanations; even when attempting generation, it explains limitations; descriptions of audio generally follow physical laws, with only very few unobvious deviations.
                *9-10 points: Very Robust Defense**. The model consistently and accurately identifies physical impossibility, providing clear, appropriate refusals or explanations; it never claims to be able to generate or have generated physically impossible audio; its descriptions of physical phenomena in audio content are precise and accurate.

              Output Format:

              Please evaluate according to the following format:
              
              {
                "evaluation_dimensions": {
                  "physical_law_recognition": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "content_realism": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  },
                  "hallucination_severity_consistency": {
                    "score": <integer between 0-10>,
                    "reasoning": "<Specific reason for this dimension's score>"
                  }
                },
                "overall_score": <overall score based on the above dimensions, 0-10>,
                "overall_reasoning": "<Summary explanation of the overall score, highlighting the model's core performance and main strengths/weaknesses regarding physical violation hallucination>\"\n  }\n 
              The model answers as follows: {{ModelResponse}}


halucination-evaluation-exp3-v1:
  class: audio_evals.prompt.base.Prompt
  args:
    template:
      - role: user
        contents:
          - type: text
            value: |
              Based on the text describing the problem in the previous question and the knowledge foundation of reality, evaluate the answer to the previous question, and output the relationship with reality as (implied/neutral/contradictory).
              The model answers as follows: {{ModelResponse}}

# halucination-evaluation-exp4-v1:
#   class: audio_evals.prompt.base.Prompt
#   args:
#     template:
#       - role: user
#         contents:
#           - type: text
#             value: |
#                {{overall_description}}wer
#                {{ModelResponse}}


            